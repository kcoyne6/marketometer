{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "# Import the NLTK package\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /Users/kingsleyogwuegbu/nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>twitter_user_id</th>\n",
       "      <th>tweet_date_time</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>917874178421874688</td>\n",
       "      <td>880412538625810432</td>\n",
       "      <td>2017-10-10 18:07:12</td>\n",
       "      <td>Be sure to watch our live webcast tonight at 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>916022934875099136</td>\n",
       "      <td>880412538625810432</td>\n",
       "      <td>2017-10-05 15:31:01</td>\n",
       "      <td>.@jimcramer I really enjoyed the interview las...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>915682435970498562</td>\n",
       "      <td>880412538625810432</td>\n",
       "      <td>2017-10-04 16:57:59</td>\n",
       "      <td>I will be on @jimcramer tonight at 6pm to disc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>910517028649422848</td>\n",
       "      <td>880412538625810432</td>\n",
       "      <td>2017-09-20 10:52:30</td>\n",
       "      <td>I’ll be on @CNBC with @ScottWapnerCNBC today t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>910497698901569543</td>\n",
       "      <td>880412538625810432</td>\n",
       "      <td>2017-09-20 09:35:42</td>\n",
       "      <td>$ADP Fellow Shareholders, please watch this vi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id     twitter_user_id      tweet_date_time  \\\n",
       "0  917874178421874688  880412538625810432  2017-10-10 18:07:12   \n",
       "1  916022934875099136  880412538625810432  2017-10-05 15:31:01   \n",
       "2  915682435970498562  880412538625810432  2017-10-04 16:57:59   \n",
       "3  910517028649422848  880412538625810432  2017-09-20 10:52:30   \n",
       "4  910497698901569543  880412538625810432  2017-09-20 09:35:42   \n",
       "\n",
       "                                          tweet_text  \n",
       "0  Be sure to watch our live webcast tonight at 7...  \n",
       "1  .@jimcramer I really enjoyed the interview las...  \n",
       "2  I will be on @jimcramer tonight at 6pm to disc...  \n",
       "3  I’ll be on @CNBC with @ScottWapnerCNBC today t...  \n",
       "4  $ADP Fellow Shareholders, please watch this vi...  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"twitter_tweet_arr_df_clean.csv\", encoding=\"utf-8\")\n",
    "df.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     Be sure to watch our live webcast tonight at 7...\n",
       "1     .@jimcramer I really enjoyed the interview las...\n",
       "2     I will be on @jimcramer tonight at 6pm to disc...\n",
       "3     I’ll be on @CNBC with @ScottWapnerCNBC today t...\n",
       "4     $ADP Fellow Shareholders, please watch this vi...\n",
       "5     Here’s our first weekly question for @ADP $ADP...\n",
       "6     Follow @ADPascending for the latest on our pro...\n",
       "7     Why in the world is #Facebook advertising on m...\n",
       "8     Archon Management, the biggest holder of $CFMS...\n",
       "9     A brief, excellent bearish report on $BSGM was...\n",
       "10    $TRXC Note today:\\n\\n BTIG believes TransEnter...\n",
       "11    We've published a bearish report on TransEnter...\n",
       "12    This is a late notice, but we published a bear...\n",
       "13    $BSGM selloff today. Sometimes it takes a few ...\n",
       "14    We just published a new $BSGM report:https://s...\n",
       "15    Some people are impressed by $BSGM CEO token i...\n",
       "16    $SOLY got the Gen 1 approval, and the very nex...\n",
       "17    More dirt on $BSGM - someone just emailed us t...\n",
       "18    We have published a new bearish report on $BSG...\n",
       "19    Everyone, the bulls, the bears, and insiders, ...\n",
       "20    $SOLY has now hit our $6 PT, in less than our ...\n",
       "21    $SOLY published a weak rebuttal to our report....\n",
       "22    We have another idea coming up that's just lik...\n",
       "23    We published a new bearish report on Soliton $...\n",
       "24    Why $SOLY will likely be a sell the news event...\n",
       "25    Good morning. We've just published a bearish r...\n",
       "26    $ABBV the next great drug short. TGT price $60...\n",
       "27    $RVLV Citron gives more detail on the future o...\n",
       "28    Citron has never commented on an IPO but  LONG...\n",
       "29    $FLT Citron exposes one of the largest Clean E...\n",
       "                            ...                        \n",
       "57    WATCH: $GS Head of Investor Relations on the f...\n",
       "58    WATCH: \"Labor costs is a key risk that fund ma...\n",
       "59    #TBT to 1979 when former $GS Senior Partner Jo...\n",
       "60    PODCAST: $GS' Jim Garman on where investors ar...\n",
       "61    Here's what has @JimCramer calling 'The Lion K...\n",
       "62    Will $FB give investors something to like? @Er...\n",
       "63    .@jimcramer makes his way to Carol Tomé’s home...\n",
       "64    Are you Team Huberty or Team Sacconaghi when i...\n",
       "65    Love it or hate it, there's a new king on Prid...\n",
       "66    Is $MSFT the holy grail of tech?https://buff.l...\n",
       "67    Read our letter to $OXY stockholders (includin...\n",
       "68    Just issued statement regarding $CZR and $ERI ...\n",
       "69    Just issued statement regarding $DVMT amended ...\n",
       "70    Just issued statement regarding $DVMT transact...\n",
       "71    Just released our 20-slide ISS presentation in...\n",
       "72    Filed complaint against $DVMT.  Read our press...\n",
       "73    $ARII announces merger. Read $IEP press releas...\n",
       "74    Read our letter to $DVMT Dell tracking stockho...\n",
       "75    Read our statement regarding Cigna ( $CI) here...\n",
       "76    Read our letter to Cigna ( $CI) stockholders (...\n",
       "77    Read our latest letter to $SD stockholders (in...\n",
       "78    Read our latest letter to $SD stockholders (in...\n",
       "79    Read our latest letter to $SD stockholders (in...\n",
       "80    Read our latest letter to $SD stockholders (in...\n",
       "81    Read our letter to the stockholders of $SD (in...\n",
       "82    Read our letter to the board of directors of S...\n",
       "83    ... which $DIS (the stock) should be rewarded ...\n",
       "84    The fact that $DIS's stock price doesn't go up...\n",
       "85    Variety: ComicCon's Big Winner is Disney+ $DIS...\n",
       "86    Brian White of Monness maintains $175 PT on $TWLO\n",
       "Name: tweet_text, Length: 87, dtype: object"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_text = df.iloc[:,-1] \n",
    "tweet_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating function to take our punc, stop words before applying to our dataframe\n",
    "\n",
    "mess = 'Sample message! Notice: it has punctuation.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S',\n",
       " 'a',\n",
       " 'm',\n",
       " 'p',\n",
       " 'l',\n",
       " 'e',\n",
       " ' ',\n",
       " 'm',\n",
       " 'e',\n",
       " 's',\n",
       " 's',\n",
       " 'a',\n",
       " 'g',\n",
       " 'e',\n",
       " ' ',\n",
       " 'N',\n",
       " 'o',\n",
       " 't',\n",
       " 'i',\n",
       " 'c',\n",
       " 'e',\n",
       " ' ',\n",
       " 'i',\n",
       " 't',\n",
       " ' ',\n",
       " 'h',\n",
       " 'a',\n",
       " 's',\n",
       " ' ',\n",
       " 'p',\n",
       " 'u',\n",
       " 'n',\n",
       " 'c',\n",
       " 't',\n",
       " 'u',\n",
       " 'a',\n",
       " 't',\n",
       " 'i',\n",
       " 'o',\n",
       " 'n']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nopunc = nopunc = [c for c in  mess if c not in string.punctuation]\n",
    "nopunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "nopunc = ''.join(nopunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sample', 'message', 'Notice', 'it', 'has', 'punctuation']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nopunc.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_mess = [word for word in nopunc.split() if word.lower() not in stopwords.words ('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sample', 'message', 'Notice', 'punctuation']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_mess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(mess):\n",
    "    \n",
    "    #remove punc\n",
    "    #remove stop words\n",
    "    #return list of clean words\n",
    "\n",
    "    nopunc = [c for c in mess if c not in string.punctuation]\n",
    "    nopunc = ''.join(nopunc)\n",
    "    return [word for word in nopunc.split() if word.lower() not in stopwords.words ('english')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>twitter_user_id</th>\n",
       "      <th>tweet_date_time</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>917874178421874688</td>\n",
       "      <td>880412538625810432</td>\n",
       "      <td>2017-10-10 18:07:12</td>\n",
       "      <td>Be sure to watch our live webcast tonight at 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>916022934875099136</td>\n",
       "      <td>880412538625810432</td>\n",
       "      <td>2017-10-05 15:31:01</td>\n",
       "      <td>.@jimcramer I really enjoyed the interview las...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>915682435970498562</td>\n",
       "      <td>880412538625810432</td>\n",
       "      <td>2017-10-04 16:57:59</td>\n",
       "      <td>I will be on @jimcramer tonight at 6pm to disc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>910517028649422848</td>\n",
       "      <td>880412538625810432</td>\n",
       "      <td>2017-09-20 10:52:30</td>\n",
       "      <td>I’ll be on @CNBC with @ScottWapnerCNBC today t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>910497698901569543</td>\n",
       "      <td>880412538625810432</td>\n",
       "      <td>2017-09-20 09:35:42</td>\n",
       "      <td>$ADP Fellow Shareholders, please watch this vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>910497240279633921</td>\n",
       "      <td>880412538625810432</td>\n",
       "      <td>2017-09-20 09:33:52</td>\n",
       "      <td>Here’s our first weekly question for @ADP $ADP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>910497024520450049</td>\n",
       "      <td>880412538625810432</td>\n",
       "      <td>2017-09-20 09:33:01</td>\n",
       "      <td>Follow @ADPascending for the latest on our pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1094330532543033351</td>\n",
       "      <td>111508605</td>\n",
       "      <td>2019-02-09 15:21:45</td>\n",
       "      <td>Why in the world is #Facebook advertising on m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1149714549945167873</td>\n",
       "      <td>2792489520</td>\n",
       "      <td>2019-07-12 12:18:04</td>\n",
       "      <td>Archon Management, the biggest holder of $CFMS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1148951339080257538</td>\n",
       "      <td>2792489520</td>\n",
       "      <td>2019-07-10 09:45:20</td>\n",
       "      <td>A brief, excellent bearish report on $BSGM was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1146051979598319618</td>\n",
       "      <td>2792489520</td>\n",
       "      <td>2019-07-02 09:44:19</td>\n",
       "      <td>$TRXC Note today:\\n\\n BTIG believes TransEnter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1144251679572987906</td>\n",
       "      <td>2792489520</td>\n",
       "      <td>2019-06-27 10:30:34</td>\n",
       "      <td>We've published a bearish report on TransEnter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1141105834048536576</td>\n",
       "      <td>2792489520</td>\n",
       "      <td>2019-06-18 18:10:06</td>\n",
       "      <td>This is a late notice, but we published a bear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1136381577741684738</td>\n",
       "      <td>2792489520</td>\n",
       "      <td>2019-06-05 17:17:35</td>\n",
       "      <td>$BSGM selloff today. Sometimes it takes a few ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1135909396293328896</td>\n",
       "      <td>2792489520</td>\n",
       "      <td>2019-06-04 10:01:18</td>\n",
       "      <td>We just published a new $BSGM report:https://s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1134516945758367744</td>\n",
       "      <td>2792489520</td>\n",
       "      <td>2019-05-31 13:48:12</td>\n",
       "      <td>Some people are impressed by $BSGM CEO token i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1134444248244391936</td>\n",
       "      <td>2792489520</td>\n",
       "      <td>2019-05-31 08:59:20</td>\n",
       "      <td>$SOLY got the Gen 1 approval, and the very nex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1133764207873277952</td>\n",
       "      <td>2792489520</td>\n",
       "      <td>2019-05-29 11:57:06</td>\n",
       "      <td>More dirt on $BSGM - someone just emailed us t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1133735339435200512</td>\n",
       "      <td>2792489520</td>\n",
       "      <td>2019-05-29 10:02:23</td>\n",
       "      <td>We have published a new bearish report on $BSG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1133458633218568198</td>\n",
       "      <td>2792489520</td>\n",
       "      <td>2019-05-28 15:42:51</td>\n",
       "      <td>Everyone, the bulls, the bears, and insiders, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1131159390336311296</td>\n",
       "      <td>2792489520</td>\n",
       "      <td>2019-05-22 07:26:29</td>\n",
       "      <td>$SOLY has now hit our $6 PT, in less than our ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1129393865306365957</td>\n",
       "      <td>2792489520</td>\n",
       "      <td>2019-05-17 10:30:55</td>\n",
       "      <td>$SOLY published a weak rebuttal to our report....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1129030991979393028</td>\n",
       "      <td>2792489520</td>\n",
       "      <td>2019-05-16 10:28:59</td>\n",
       "      <td>We have another idea coming up that's just lik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1129023728472526848</td>\n",
       "      <td>2792489520</td>\n",
       "      <td>2019-05-16 10:00:07</td>\n",
       "      <td>We published a new bearish report on Soliton $...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1126959180785618945</td>\n",
       "      <td>2792489520</td>\n",
       "      <td>2019-05-10 17:16:21</td>\n",
       "      <td>Why $SOLY will likely be a sell the news event...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1126487012264087552</td>\n",
       "      <td>2792489520</td>\n",
       "      <td>2019-05-09 10:00:07</td>\n",
       "      <td>Good morning. We've just published a bearish r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1019941724192325634</td>\n",
       "      <td>236953420</td>\n",
       "      <td>2018-07-19 09:47:11</td>\n",
       "      <td>$ABBV the next great drug short. TGT price $60...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1138078829241544705</td>\n",
       "      <td>236953420</td>\n",
       "      <td>2019-06-10 09:41:52</td>\n",
       "      <td>$RVLV Citron gives more detail on the future o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1137018178553257984</td>\n",
       "      <td>236953420</td>\n",
       "      <td>2019-06-07 11:27:13</td>\n",
       "      <td>Citron has never commented on an IPO but  LONG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1136642469666250752</td>\n",
       "      <td>236953420</td>\n",
       "      <td>2019-06-06 10:34:17</td>\n",
       "      <td>$FLT Citron exposes one of the largest Clean E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>1151959952031932424</td>\n",
       "      <td>253167239</td>\n",
       "      <td>2019-07-18 17:00:29</td>\n",
       "      <td>WATCH: $GS Head of Investor Relations on the f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1151891960766050305</td>\n",
       "      <td>253167239</td>\n",
       "      <td>2019-07-18 12:30:19</td>\n",
       "      <td>WATCH: \"Labor costs is a key risk that fund ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>1151854279713538048</td>\n",
       "      <td>253167239</td>\n",
       "      <td>2019-07-18 10:00:35</td>\n",
       "      <td>#TBT to 1979 when former $GS Senior Partner Jo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1151824342826852352</td>\n",
       "      <td>253167239</td>\n",
       "      <td>2019-07-18 08:01:37</td>\n",
       "      <td>PODCAST: $GS' Jim Garman on where investors ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>1153366095266340868</td>\n",
       "      <td>14216123</td>\n",
       "      <td>2019-07-22 14:08:00</td>\n",
       "      <td>Here's what has @JimCramer calling 'The Lion K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>1153283811435827203</td>\n",
       "      <td>14216123</td>\n",
       "      <td>2019-07-22 08:41:02</td>\n",
       "      <td>Will $FB give investors something to like? @Er...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>1153402723716517891</td>\n",
       "      <td>14216123</td>\n",
       "      <td>2019-07-22 16:33:33</td>\n",
       "      <td>.@jimcramer makes his way to Carol Tomé’s home...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>1153345208186281985</td>\n",
       "      <td>14216123</td>\n",
       "      <td>2019-07-22 12:45:00</td>\n",
       "      <td>Are you Team Huberty or Team Sacconaghi when i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>1153334905167929344</td>\n",
       "      <td>14216123</td>\n",
       "      <td>2019-07-22 12:04:03</td>\n",
       "      <td>Love it or hate it, there's a new king on Prid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>1153282044266520576</td>\n",
       "      <td>14216123</td>\n",
       "      <td>2019-07-22 08:34:00</td>\n",
       "      <td>Is $MSFT the holy grail of tech?https://buff.l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>1153356622804205569</td>\n",
       "      <td>1534167900</td>\n",
       "      <td>2019-07-22 13:30:21</td>\n",
       "      <td>Read our letter to $OXY stockholders (includin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>1143119516609798144</td>\n",
       "      <td>1534167900</td>\n",
       "      <td>2019-06-24 07:31:45</td>\n",
       "      <td>Just issued statement regarding $CZR and $ERI ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>1063182759701684232</td>\n",
       "      <td>1534167900</td>\n",
       "      <td>2018-11-15 16:31:37</td>\n",
       "      <td>Just issued statement regarding $DVMT amended ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>1062801937035419648</td>\n",
       "      <td>1534167900</td>\n",
       "      <td>2018-11-14 15:18:22</td>\n",
       "      <td>Just issued statement regarding $DVMT transact...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1061951210289352704</td>\n",
       "      <td>1534167900</td>\n",
       "      <td>2018-11-12 06:57:53</td>\n",
       "      <td>Just released our 20-slide ISS presentation in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>1057950859626274816</td>\n",
       "      <td>1534167900</td>\n",
       "      <td>2018-11-01 07:01:55</td>\n",
       "      <td>Filed complaint against $DVMT.  Read our press...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>1054329292115636224</td>\n",
       "      <td>1534167900</td>\n",
       "      <td>2018-10-22 07:11:06</td>\n",
       "      <td>$ARII announces merger. Read $IEP press releas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1051775182610804736</td>\n",
       "      <td>1534167900</td>\n",
       "      <td>2018-10-15 06:01:59</td>\n",
       "      <td>Read our letter to $DVMT Dell tracking stockho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>1029147878613241862</td>\n",
       "      <td>1534167900</td>\n",
       "      <td>2018-08-13 19:29:09</td>\n",
       "      <td>Read our statement regarding Cigna ( $CI) here...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>1026770474460766208</td>\n",
       "      <td>1534167900</td>\n",
       "      <td>2018-08-07 06:02:12</td>\n",
       "      <td>Read our letter to Cigna ( $CI) stockholders (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>1008805094295470081</td>\n",
       "      <td>1534167900</td>\n",
       "      <td>2018-06-18 16:14:11</td>\n",
       "      <td>Read our latest letter to $SD stockholders (in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1008658533586538496</td>\n",
       "      <td>1534167900</td>\n",
       "      <td>2018-06-18 06:31:49</td>\n",
       "      <td>Read our latest letter to $SD stockholders (in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1007679084791582726</td>\n",
       "      <td>1534167900</td>\n",
       "      <td>2018-06-15 13:39:50</td>\n",
       "      <td>Read our latest letter to $SD stockholders (in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1007247033781145600</td>\n",
       "      <td>1534167900</td>\n",
       "      <td>2018-06-14 09:03:01</td>\n",
       "      <td>Read our latest letter to $SD stockholders (in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>1006159774147399680</td>\n",
       "      <td>1534167900</td>\n",
       "      <td>2018-06-11 09:02:38</td>\n",
       "      <td>Read our letter to the stockholders of $SD (in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1004680338575187968</td>\n",
       "      <td>1534167900</td>\n",
       "      <td>2018-06-07 07:03:53</td>\n",
       "      <td>Read our letter to the board of directors of S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>1153392552822923265</td>\n",
       "      <td>818071</td>\n",
       "      <td>2019-07-22 15:53:08</td>\n",
       "      <td>... which $DIS (the stock) should be rewarded ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>1153390620947484672</td>\n",
       "      <td>818071</td>\n",
       "      <td>2019-07-22 15:45:27</td>\n",
       "      <td>The fact that $DIS's stock price doesn't go up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>1153347311101272066</td>\n",
       "      <td>818071</td>\n",
       "      <td>2019-07-22 12:53:21</td>\n",
       "      <td>Variety: ComicCon's Big Winner is Disney+ $DIS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1153282741842206722</td>\n",
       "      <td>818071</td>\n",
       "      <td>2019-07-22 08:36:47</td>\n",
       "      <td>Brian White of Monness maintains $175 PT on $TWLO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               tweet_id     twitter_user_id      tweet_date_time  \\\n",
       "0    917874178421874688  880412538625810432  2017-10-10 18:07:12   \n",
       "1    916022934875099136  880412538625810432  2017-10-05 15:31:01   \n",
       "2    915682435970498562  880412538625810432  2017-10-04 16:57:59   \n",
       "3    910517028649422848  880412538625810432  2017-09-20 10:52:30   \n",
       "4    910497698901569543  880412538625810432  2017-09-20 09:35:42   \n",
       "5    910497240279633921  880412538625810432  2017-09-20 09:33:52   \n",
       "6    910497024520450049  880412538625810432  2017-09-20 09:33:01   \n",
       "7   1094330532543033351           111508605  2019-02-09 15:21:45   \n",
       "8   1149714549945167873          2792489520  2019-07-12 12:18:04   \n",
       "9   1148951339080257538          2792489520  2019-07-10 09:45:20   \n",
       "10  1146051979598319618          2792489520  2019-07-02 09:44:19   \n",
       "11  1144251679572987906          2792489520  2019-06-27 10:30:34   \n",
       "12  1141105834048536576          2792489520  2019-06-18 18:10:06   \n",
       "13  1136381577741684738          2792489520  2019-06-05 17:17:35   \n",
       "14  1135909396293328896          2792489520  2019-06-04 10:01:18   \n",
       "15  1134516945758367744          2792489520  2019-05-31 13:48:12   \n",
       "16  1134444248244391936          2792489520  2019-05-31 08:59:20   \n",
       "17  1133764207873277952          2792489520  2019-05-29 11:57:06   \n",
       "18  1133735339435200512          2792489520  2019-05-29 10:02:23   \n",
       "19  1133458633218568198          2792489520  2019-05-28 15:42:51   \n",
       "20  1131159390336311296          2792489520  2019-05-22 07:26:29   \n",
       "21  1129393865306365957          2792489520  2019-05-17 10:30:55   \n",
       "22  1129030991979393028          2792489520  2019-05-16 10:28:59   \n",
       "23  1129023728472526848          2792489520  2019-05-16 10:00:07   \n",
       "24  1126959180785618945          2792489520  2019-05-10 17:16:21   \n",
       "25  1126487012264087552          2792489520  2019-05-09 10:00:07   \n",
       "26  1019941724192325634           236953420  2018-07-19 09:47:11   \n",
       "27  1138078829241544705           236953420  2019-06-10 09:41:52   \n",
       "28  1137018178553257984           236953420  2019-06-07 11:27:13   \n",
       "29  1136642469666250752           236953420  2019-06-06 10:34:17   \n",
       "..                  ...                 ...                  ...   \n",
       "57  1151959952031932424           253167239  2019-07-18 17:00:29   \n",
       "58  1151891960766050305           253167239  2019-07-18 12:30:19   \n",
       "59  1151854279713538048           253167239  2019-07-18 10:00:35   \n",
       "60  1151824342826852352           253167239  2019-07-18 08:01:37   \n",
       "61  1153366095266340868            14216123  2019-07-22 14:08:00   \n",
       "62  1153283811435827203            14216123  2019-07-22 08:41:02   \n",
       "63  1153402723716517891            14216123  2019-07-22 16:33:33   \n",
       "64  1153345208186281985            14216123  2019-07-22 12:45:00   \n",
       "65  1153334905167929344            14216123  2019-07-22 12:04:03   \n",
       "66  1153282044266520576            14216123  2019-07-22 08:34:00   \n",
       "67  1153356622804205569          1534167900  2019-07-22 13:30:21   \n",
       "68  1143119516609798144          1534167900  2019-06-24 07:31:45   \n",
       "69  1063182759701684232          1534167900  2018-11-15 16:31:37   \n",
       "70  1062801937035419648          1534167900  2018-11-14 15:18:22   \n",
       "71  1061951210289352704          1534167900  2018-11-12 06:57:53   \n",
       "72  1057950859626274816          1534167900  2018-11-01 07:01:55   \n",
       "73  1054329292115636224          1534167900  2018-10-22 07:11:06   \n",
       "74  1051775182610804736          1534167900  2018-10-15 06:01:59   \n",
       "75  1029147878613241862          1534167900  2018-08-13 19:29:09   \n",
       "76  1026770474460766208          1534167900  2018-08-07 06:02:12   \n",
       "77  1008805094295470081          1534167900  2018-06-18 16:14:11   \n",
       "78  1008658533586538496          1534167900  2018-06-18 06:31:49   \n",
       "79  1007679084791582726          1534167900  2018-06-15 13:39:50   \n",
       "80  1007247033781145600          1534167900  2018-06-14 09:03:01   \n",
       "81  1006159774147399680          1534167900  2018-06-11 09:02:38   \n",
       "82  1004680338575187968          1534167900  2018-06-07 07:03:53   \n",
       "83  1153392552822923265              818071  2019-07-22 15:53:08   \n",
       "84  1153390620947484672              818071  2019-07-22 15:45:27   \n",
       "85  1153347311101272066              818071  2019-07-22 12:53:21   \n",
       "86  1153282741842206722              818071  2019-07-22 08:36:47   \n",
       "\n",
       "                                           tweet_text  \n",
       "0   Be sure to watch our live webcast tonight at 7...  \n",
       "1   .@jimcramer I really enjoyed the interview las...  \n",
       "2   I will be on @jimcramer tonight at 6pm to disc...  \n",
       "3   I’ll be on @CNBC with @ScottWapnerCNBC today t...  \n",
       "4   $ADP Fellow Shareholders, please watch this vi...  \n",
       "5   Here’s our first weekly question for @ADP $ADP...  \n",
       "6   Follow @ADPascending for the latest on our pro...  \n",
       "7   Why in the world is #Facebook advertising on m...  \n",
       "8   Archon Management, the biggest holder of $CFMS...  \n",
       "9   A brief, excellent bearish report on $BSGM was...  \n",
       "10  $TRXC Note today:\\n\\n BTIG believes TransEnter...  \n",
       "11  We've published a bearish report on TransEnter...  \n",
       "12  This is a late notice, but we published a bear...  \n",
       "13  $BSGM selloff today. Sometimes it takes a few ...  \n",
       "14  We just published a new $BSGM report:https://s...  \n",
       "15  Some people are impressed by $BSGM CEO token i...  \n",
       "16  $SOLY got the Gen 1 approval, and the very nex...  \n",
       "17  More dirt on $BSGM - someone just emailed us t...  \n",
       "18  We have published a new bearish report on $BSG...  \n",
       "19  Everyone, the bulls, the bears, and insiders, ...  \n",
       "20  $SOLY has now hit our $6 PT, in less than our ...  \n",
       "21  $SOLY published a weak rebuttal to our report....  \n",
       "22  We have another idea coming up that's just lik...  \n",
       "23  We published a new bearish report on Soliton $...  \n",
       "24  Why $SOLY will likely be a sell the news event...  \n",
       "25  Good morning. We've just published a bearish r...  \n",
       "26  $ABBV the next great drug short. TGT price $60...  \n",
       "27  $RVLV Citron gives more detail on the future o...  \n",
       "28  Citron has never commented on an IPO but  LONG...  \n",
       "29  $FLT Citron exposes one of the largest Clean E...  \n",
       "..                                                ...  \n",
       "57  WATCH: $GS Head of Investor Relations on the f...  \n",
       "58  WATCH: \"Labor costs is a key risk that fund ma...  \n",
       "59  #TBT to 1979 when former $GS Senior Partner Jo...  \n",
       "60  PODCAST: $GS' Jim Garman on where investors ar...  \n",
       "61  Here's what has @JimCramer calling 'The Lion K...  \n",
       "62  Will $FB give investors something to like? @Er...  \n",
       "63  .@jimcramer makes his way to Carol Tomé’s home...  \n",
       "64  Are you Team Huberty or Team Sacconaghi when i...  \n",
       "65  Love it or hate it, there's a new king on Prid...  \n",
       "66  Is $MSFT the holy grail of tech?https://buff.l...  \n",
       "67  Read our letter to $OXY stockholders (includin...  \n",
       "68  Just issued statement regarding $CZR and $ERI ...  \n",
       "69  Just issued statement regarding $DVMT amended ...  \n",
       "70  Just issued statement regarding $DVMT transact...  \n",
       "71  Just released our 20-slide ISS presentation in...  \n",
       "72  Filed complaint against $DVMT.  Read our press...  \n",
       "73  $ARII announces merger. Read $IEP press releas...  \n",
       "74  Read our letter to $DVMT Dell tracking stockho...  \n",
       "75  Read our statement regarding Cigna ( $CI) here...  \n",
       "76  Read our letter to Cigna ( $CI) stockholders (...  \n",
       "77  Read our latest letter to $SD stockholders (in...  \n",
       "78  Read our latest letter to $SD stockholders (in...  \n",
       "79  Read our latest letter to $SD stockholders (in...  \n",
       "80  Read our latest letter to $SD stockholders (in...  \n",
       "81  Read our letter to the stockholders of $SD (in...  \n",
       "82  Read our letter to the board of directors of S...  \n",
       "83  ... which $DIS (the stock) should be rewarded ...  \n",
       "84  The fact that $DIS's stock price doesn't go up...  \n",
       "85  Variety: ComicCon's Big Winner is Disney+ $DIS...  \n",
       "86  Brian White of Monness maintains $175 PT on $TWLO  \n",
       "\n",
       "[87 rows x 4 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Apply function created above to our original dataframe to tokenize it and get clean verison\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [sure, watch, live, webcast, tonight, 7PM, ET,...\n",
       "1     [jimcramer, really, enjoyed, interview, last, ...\n",
       "2     [jimcramer, tonight, 6pm, discuss, ADP, Hope, ...\n",
       "3     [I’ll, CNBC, ScottWapnerCNBC, today, discuss, ...\n",
       "4     [ADP, Fellow, Shareholders, please, watch, vid...\n",
       "5     [Here’s, first, weekly, question, ADP, ADP, SE...\n",
       "6     [Follow, ADPascending, latest, proxy, contest,...\n",
       "7     [world, Facebook, advertising, mobile, keep, s...\n",
       "8     [Archon, Management, biggest, holder, CFMS, du...\n",
       "9     [brief, excellent, bearish, report, BSGM, post...\n",
       "10    [TRXC, Note, today, BTIG, believes, TransEnter...\n",
       "11    [Weve, published, bearish, report, TransEnteri...\n",
       "12    [late, notice, published, bearish, report, App...\n",
       "13    [BSGM, selloff, today, Sometimes, takes, days,...\n",
       "14    [published, new, BSGM, reporthttpsseekingalpha...\n",
       "15    [people, impressed, BSGM, CEO, token, insider,...\n",
       "16    [SOLY, got, Gen, 1, approval, next, day, annou...\n",
       "17    [dirt, BSGM, someone, emailed, us, today, info...\n",
       "18    [published, new, bearish, report, BSGMhttpssee...\n",
       "19    [Everyone, bulls, bears, insiders, knew, SOLYs...\n",
       "20    [SOLY, hit, 6, PT, less, 2, week, time, frame,...\n",
       "21    [SOLY, published, weak, rebuttal, report, supp...\n",
       "22    [another, idea, coming, thats, like, SOLY, wou...\n",
       "23    [published, new, bearish, report, Soliton, SOL...\n",
       "24    [SOLY, likely, sell, news, event, Monday, http...\n",
       "25    [Good, morning, Weve, published, bearish, repo...\n",
       "26    [ABBV, next, great, drug, short, TGT, price, 6...\n",
       "27    [RVLV, Citron, gives, detail, future, FashTech...\n",
       "28    [Citron, never, commented, IPO, LONG, RVLV, Pr...\n",
       "29    [FLT, Citron, exposes, one, largest, Clean, En...\n",
       "                            ...                        \n",
       "57    [WATCH, GS, Head, Investor, Relations, firm’s,...\n",
       "58    [WATCH, Labor, costs, key, risk, fund, manager...\n",
       "59    [TBT, 1979, former, GS, Senior, Partner, John,...\n",
       "60    [PODCAST, GS, Jim, Garman, investors, finding,...\n",
       "61    [Heres, JimCramer, calling, Lion, King, metaph...\n",
       "62    [FB, give, investors, something, like, EricJho...\n",
       "63    [jimcramer, makes, way, Carol, Tomé’s, home, t...\n",
       "64    [Team, Huberty, Team, Sacconaghi, comes, AAPL,...\n",
       "65    [Love, hate, theres, new, king, Pride, Rock, J...\n",
       "66          [MSFT, holy, grail, techhttpsbuffly2xWhsg7]\n",
       "67    [Read, letter, OXY, stockholders, including, i...\n",
       "68    [issued, statement, regarding, CZR, ERI, merge...\n",
       "69    [issued, statement, regarding, DVMT, amended, ...\n",
       "70    [issued, statement, regarding, DVMT, transacti...\n",
       "71    [released, 20slide, ISS, presentation, opposit...\n",
       "72    [Filed, complaint, DVMT, Read, press, release,...\n",
       "73    [ARII, announces, merger, Read, IEP, press, re...\n",
       "74    [Read, letter, DVMT, Dell, tracking, stockhold...\n",
       "75    [Read, statement, regarding, Cigna, CI, httpsc...\n",
       "76    [Read, letter, Cigna, CI, stockholders, includ...\n",
       "77    [Read, latest, letter, SD, stockholders, inclu...\n",
       "78    [Read, latest, letter, SD, stockholders, inclu...\n",
       "79    [Read, latest, letter, SD, stockholders, inclu...\n",
       "80    [Read, latest, letter, SD, stockholders, inclu...\n",
       "81    [Read, letter, stockholders, SD, including, im...\n",
       "82    [Read, letter, board, directors, SandRidge, En...\n",
       "83    [DIS, stock, rewarded, higher, multiple, inves...\n",
       "84    [fact, DISs, stock, price, doesnt, go, Lion, K...\n",
       "85    [Variety, ComicCons, Big, Winner, Disney, DISh...\n",
       "86    [Brian, White, Monness, maintains, 175, PT, TWLO]\n",
       "Name: tweet_text, Length: 87, dtype: object"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenize the text\n",
    "\n",
    "df['tweet_text'].apply(text_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert text into vector that ML models can understand.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_transformer = CountVectorizer(analyzer = text_process).fit(df['tweet_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "918\n"
     ]
    }
   ],
   "source": [
    "#print the length of vocabulary words\n",
    "print(len(bow_transformer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bow = bow_transformer.transform(df['tweet_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine document similarity using TF-IDF\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer().fit(df_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf = tfidf_transformer.transform(df_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training our classifier\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_model = MultinomialNB().fit(df_tfidf,df['tweet_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training/Splitting our Data\n",
    "\n",
    "df_train, df_test, label_train,label_test = train_test_split(df['tweet_text'],df['twitter_user_id'],test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "        ('bow', CountVectorizer(analyzer = text_process)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('classifier', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('bow',\n",
       "                 CountVectorizer(analyzer=<function text_process at 0x1a17bc91e0>,\n",
       "                                 binary=False, decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('classifier',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(df_train,label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification Report\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "            818071       0.00      0.00      0.00         1\n",
      "          14216123       0.00      0.00      0.00         2\n",
      "         236953420       0.62      1.00      0.77         5\n",
      "         253167239       1.00      0.86      0.92         7\n",
      "        1534167900       1.00      1.00      1.00         6\n",
      "        2792489520       0.71      1.00      0.83         5\n",
      "880412538625810432       0.00      0.00      0.00         1\n",
      "\n",
      "          accuracy                           0.81        27\n",
      "         macro avg       0.48      0.55      0.50        27\n",
      "      weighted avg       0.73      0.81      0.76        27\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kingsleyogwuegbu/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(label_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
